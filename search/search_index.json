{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"PyGPT4All API Reference","text":""},{"location":"#pygpt4all.models.gpt4all","title":"pygpt4all.models.gpt4all","text":"<p>GPT4ALL with <code>llama.cpp</code> backend through pyllamacpp</p>"},{"location":"#pygpt4all.models.gpt4all.GPT4All","title":"GPT4All","text":"<pre><code>GPT4All(\n    model_path,\n    prompt_context=\"\",\n    prompt_prefix=\"\",\n    prompt_suffix=\"\",\n    log_level=logging.ERROR,\n    n_ctx=512,\n    seed=0,\n    n_parts=-1,\n    f16_kv=False,\n    logits_all=False,\n    vocab_only=False,\n    use_mlock=False,\n    embedding=False,\n)\n</code></pre> <p>         Bases: <code>pyllamacpp.model.Model</code></p> <p>GPT4All model</p> <p>Base: pyllamacpp.model.Model</p> <p>Example usage <pre><code>from pygpt4all.models.gpt4all import GPT4All\n\nmodel = GPT4All('path/to/gpt4all/model')\nfor token in model.generate(\"Tell me a joke ?\"):\n    print(token, end='', flush=True)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>the path to the gpt4all model</p> required <code>prompt_context</code> <code>str</code> <p>the global context of the interaction</p> <code>''</code> <code>prompt_prefix</code> <code>str</code> <p>the prompt prefix</p> <code>''</code> <code>prompt_suffix</code> <code>str</code> <p>the prompt suffix</p> <code>''</code> <code>log_level</code> <code>int</code> <p>logging level, set to ERROR by default</p> <code>logging.ERROR</code> <code>n_ctx</code> <code>int</code> <p>LLaMA context</p> <code>512</code> <code>seed</code> <code>int</code> <p>random seed</p> <code>0</code> <code>n_parts</code> <code>int</code> <p>LLaMA n_parts</p> <code>-1</code> <code>f16_kv</code> <code>bool</code> <p>use fp16 for KV cache</p> <code>False</code> <code>logits_all</code> <code>bool</code> <p>the llama_eval() call computes all logits, not just the last one</p> <code>False</code> <code>vocab_only</code> <code>bool</code> <p>only load the vocabulary, no weights</p> <code>False</code> <code>use_mlock</code> <code>bool</code> <p>force system to keep model in RAM</p> <code>False</code> <code>embedding</code> <code>bool</code> <p>embedding mode only</p> <code>False</code> Source code in <code>pygpt4all/models/gpt4all.py</code> <pre><code>def __init__(self,\n             model_path: str,\n             prompt_context: str = '',\n             prompt_prefix: str = '',\n             prompt_suffix: str = '',\n             log_level: int = logging.ERROR,\n             n_ctx: int = 512,\n             seed: int = 0,\n             n_parts: int = -1,\n             f16_kv: bool = False,\n             logits_all: bool = False,\n             vocab_only: bool = False,\n             use_mlock: bool = False,\n             embedding: bool = False):\n\"\"\"\n    :param model_path: the path to the gpt4all model\n    :param prompt_context: the global context of the interaction\n    :param prompt_prefix: the prompt prefix\n    :param prompt_suffix: the prompt suffix\n    :param log_level: logging level, set to ERROR by default\n    :param n_ctx: LLaMA context\n    :param seed: random seed\n    :param n_parts: LLaMA n_parts\n    :param f16_kv: use fp16 for KV cache\n    :param logits_all: the llama_eval() call computes all logits, not just the last one\n    :param vocab_only: only load the vocabulary, no weights\n    :param use_mlock: force system to keep model in RAM\n    :param embedding: embedding mode only\n    \"\"\"\n    # set logging level\n    set_log_level(log_level)\n    super(GPT4All, self).__init__(model_path=model_path,\n                                  prompt_context=prompt_context,\n                                  prompt_prefix=prompt_prefix,\n                                  prompt_suffix=prompt_suffix,\n                                  log_level=log_level,\n                                  n_ctx=n_ctx,\n                                  seed=seed,\n                                  n_parts=n_parts,\n                                  f16_kv=f16_kv,\n                                  logits_all=logits_all,\n                                  vocab_only=vocab_only,\n                                  use_mlock=use_mlock,\n                                  embedding=embedding)\n</code></pre>"},{"location":"#pygpt4all.models.gpt4all_j","title":"pygpt4all.models.gpt4all_j","text":"<p>GPT4ALL with <code>ggml</code> backend</p>"},{"location":"#pygpt4all.models.gpt4all_j.GPT4All_J","title":"GPT4All_J","text":"<pre><code>GPT4All_J(\n    model_path,\n    prompt_context=\"\",\n    prompt_prefix=\"\",\n    prompt_suffix=\"\",\n    log_level=logging.ERROR,\n)\n</code></pre> <p>         Bases: <code>pygptj.model.Model</code></p> <p>GPT4ALL-J model</p> <p>Example usage <pre><code>from pygpt4all.models.gpt4all_j import GPT4All_J\n\nmodel = GPT4All_J('.path/to/gpr4all-j/model')\nfor token in model.generate(\"Tell me a joke ?\"):\n    print(token, end='', flush=True)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>The path to a gpt4all-j model</p> required <code>prompt_context</code> <code>str</code> <p>the global context of the interaction</p> <code>''</code> <code>prompt_prefix</code> <code>str</code> <p>the prompt prefix</p> <code>''</code> <code>prompt_suffix</code> <code>str</code> <p>the prompt suffix</p> <code>''</code> <code>log_level</code> <code>int</code> <p>logging level, set to ERROR by default</p> <code>logging.ERROR</code> Source code in <code>pygpt4all/models/gpt4all_j.py</code> <pre><code>def __init__(self,\n             model_path: str,\n             prompt_context: str = '',\n             prompt_prefix: str = '',\n             prompt_suffix: str = '',\n             log_level: int = logging.ERROR):\n\"\"\"\n    :param model_path: The path to a gpt4all-j model\n    :param prompt_context: the global context of the interaction\n    :param prompt_prefix: the prompt prefix\n    :param prompt_suffix: the prompt suffix\n    :param log_level: logging level, set to ERROR by default\n    \"\"\"\n    # set logging level\n    set_log_level(log_level)\n    super(GPT4All_J, self).__init__(model_path=model_path,\n                                    prompt_context=prompt_context,\n                                    prompt_prefix=prompt_prefix,\n                                    prompt_suffix=prompt_suffix,\n                                    log_level=log_level)\n</code></pre>"},{"location":"#bases","title":"Bases","text":""},{"location":"#pyllamacpp.model","title":"pyllamacpp.model","text":"<p>This module contains a simple Python API around llama.cpp</p>"},{"location":"#pyllamacpp.model.Model","title":"Model","text":"<pre><code>Model(\n    model_path,\n    prompt_context=\"\",\n    prompt_prefix=\"\",\n    prompt_suffix=\"\",\n    log_level=logging.ERROR,\n    n_ctx=512,\n    seed=0,\n    n_parts=-1,\n    f16_kv=False,\n    logits_all=False,\n    vocab_only=False,\n    use_mlock=False,\n    embedding=False,\n)\n</code></pre> <p>A simple Python class on top of llama.cpp</p> <p>Example usage <pre><code>from pyllamacpp.model import Model\n\nmodel = Model(ggml_model='path/to/ggml/model')\nfor token in model.generate(\"Tell me a joke ?\"):\n    print(token, end='', flush=True)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>the path to the ggml model</p> required <code>prompt_context</code> <code>str</code> <p>the global context of the interaction</p> <code>''</code> <code>prompt_prefix</code> <code>str</code> <p>the prompt prefix</p> <code>''</code> <code>prompt_suffix</code> <code>str</code> <p>the prompt suffix</p> <code>''</code> <code>log_level</code> <code>int</code> <p>logging level, set to INFO by default</p> <code>logging.ERROR</code> <code>n_ctx</code> <code>int</code> <p>LLaMA context</p> <code>512</code> <code>seed</code> <code>int</code> <p>random seed</p> <code>0</code> <code>n_parts</code> <code>int</code> <p>LLaMA n_parts</p> <code>-1</code> <code>f16_kv</code> <code>bool</code> <p>use fp16 for KV cache</p> <code>False</code> <code>logits_all</code> <code>bool</code> <p>the llama_eval() call computes all logits, not just the last one</p> <code>False</code> <code>vocab_only</code> <code>bool</code> <p>only load the vocabulary, no weights</p> <code>False</code> <code>use_mlock</code> <code>bool</code> <p>force system to keep model in RAM</p> <code>False</code> <code>embedding</code> <code>bool</code> <p>embedding mode only</p> <code>False</code> Source code in <code>/opt/hostedtoolcache/Python/3.11.3/x64/lib/python3.11/site-packages/pyllamacpp/model.py</code> <pre><code>def __init__(self,\n             model_path: str,\n             prompt_context: str = '',\n             prompt_prefix: str = '',\n             prompt_suffix: str = '',\n             log_level: int = logging.ERROR,\n             n_ctx: int = 512,\n             seed: int = 0,\n             n_parts: int = -1,\n             f16_kv: bool = False,\n             logits_all: bool = False,\n             vocab_only: bool = False,\n             use_mlock: bool = False,\n             embedding: bool = False):\n\"\"\"\n    :param model_path: the path to the ggml model\n    :param prompt_context: the global context of the interaction\n    :param prompt_prefix: the prompt prefix\n    :param prompt_suffix: the prompt suffix\n    :param log_level: logging level, set to INFO by default\n    :param n_ctx: LLaMA context\n    :param seed: random seed\n    :param n_parts: LLaMA n_parts\n    :param f16_kv: use fp16 for KV cache\n    :param logits_all: the llama_eval() call computes all logits, not just the last one\n    :param vocab_only: only load the vocabulary, no weights\n    :param use_mlock: force system to keep model in RAM\n    :param embedding: embedding mode only\n    \"\"\"\n\n    # set logging level\n    set_log_level(log_level)\n    self._ctx = None\n\n    if not Path(model_path).is_file():\n        raise Exception(f\"File {model_path} not found!\")\n\n    self.llama_params = pp.llama_context_default_params()\n    # update llama_params\n    self.llama_params.n_ctx = n_ctx\n    self.llama_params.seed = seed\n    self.llama_params.n_parts = n_parts\n    self.llama_params.f16_kv = f16_kv\n    self.llama_params.logits_all = logits_all\n    self.llama_params.vocab_only = vocab_only\n    self.llama_params.use_mlock = use_mlock\n    self.llama_params.embedding = embedding\n\n    self._ctx = pp.llama_init_from_file(model_path, self.llama_params)\n\n    # gpt params\n    self.gpt_params = pp.gpt_params()\n\n    self.res = \"\"\n\n    self._n_ctx = pp.llama_n_ctx(self._ctx)\n    self._last_n_tokens = [0] * self._n_ctx  # n_ctx elements\n    self._n_past = 0\n    self.prompt_cntext = prompt_context\n    self.prompt_prefix = prompt_prefix\n    self.prompt_suffix = prompt_suffix\n\n    self._prompt_context_tokens = []\n    self._prompt_prefix_tokens = []\n    self._prompt_suffix_tokens = []\n\n    self.reset()\n</code></pre>"},{"location":"#pyllamacpp.model.Model.reset","title":"reset","text":"<pre><code>reset()\n</code></pre> <p>Resets the context</p> Source code in <code>/opt/hostedtoolcache/Python/3.11.3/x64/lib/python3.11/site-packages/pyllamacpp/model.py</code> <pre><code>def reset(self) -&gt; None:\n\"\"\"Resets the context\"\"\"\n    self._prompt_context_tokens = pp.llama_tokenize(self._ctx, self.prompt_cntext, True)\n    self._prompt_prefix_tokens = pp.llama_tokenize(self._ctx, self.prompt_prefix, True)\n    self._prompt_suffix_tokens = pp.llama_tokenize(self._ctx, self.prompt_suffix, True)\n    self._last_n_tokens = [0] * self._n_ctx  # n_ctx elements\n    self._n_past = 0\n</code></pre>"},{"location":"#pyllamacpp.model.Model.tokenize","title":"tokenize","text":"<pre><code>tokenize(text)\n</code></pre> <p>Returns a list of tokens for the text</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>text to be tokenized</p> required <p>Returns:</p> Type Description <p>List of tokens</p> Source code in <code>/opt/hostedtoolcache/Python/3.11.3/x64/lib/python3.11/site-packages/pyllamacpp/model.py</code> <pre><code>def tokenize(self, text:str):\n\"\"\"\n    Returns a list of tokens for the text\n    :param text: text to be tokenized\n    :return: List of tokens\n    \"\"\"\n    return pp.llama_tokenize(self._ctx, text, True)\n</code></pre>"},{"location":"#pyllamacpp.model.Model.detokenize","title":"detokenize","text":"<pre><code>detokenize(tokens)\n</code></pre> <p>Returns a list of tokens for the text</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <p>text to be tokenized</p> required <p>Returns:</p> Type Description <p>A string representing the text extracted from the tokens</p> Source code in <code>/opt/hostedtoolcache/Python/3.11.3/x64/lib/python3.11/site-packages/pyllamacpp/model.py</code> <pre><code>def detokenize(self, tokens:list):\n\"\"\"\n    Returns a list of tokens for the text\n    :param text: text to be tokenized\n    :return: A string representing the text extracted from the tokens\n    \"\"\"\n    return pp.llama_tokens_to_str(self._ctx, tokens)\n</code></pre>"},{"location":"#pyllamacpp.model.Model.generate","title":"generate","text":"<pre><code>generate(\n    prompt,\n    n_predict=None,\n    antiprompt=None,\n    infinite_generation=False,\n    n_threads=4,\n    repeat_last_n=64,\n    top_k=40,\n    top_p=0.95,\n    temp=0.8,\n    repeat_penalty=1.1,\n)\n</code></pre> <p>Runs llama.cpp inference and yields new predicted tokens from the prompt provided as input</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The prompt :)</p> required <code>n_predict</code> <code>Union[None, int]</code> <p>if n_predict is not None, the inference will stop if it reaches <code>n_predict</code> tokens, otherwise it will continue until <code>EOS</code></p> <code>None</code> <code>antiprompt</code> <code>str</code> <p>aka the stop word, the generation will stop if this word is predicted, keep it None to handle it in your own way</p> <code>None</code> <code>infinite_generation</code> <code>bool</code> <p>set it to <code>True</code> to make the generation go infinitely</p> <code>False</code> <code>n_threads</code> <code>int</code> <p>The number of CPU threads</p> <code>4</code> <code>repeat_last_n</code> <code>int</code> <p>last n tokens to penalize</p> <code>64</code> <code>top_k</code> <code>int</code> <p>top K sampling parameter</p> <code>40</code> <code>top_p</code> <code>float</code> <p>top P sampling parameter</p> <code>0.95</code> <code>temp</code> <code>float</code> <p>temperature</p> <code>0.8</code> <code>repeat_penalty</code> <code>float</code> <p>repeat penalty sampling parameter</p> <code>1.1</code> <p>Returns:</p> Type Description <code>Generator</code> <p>Tokens generator</p> Source code in <code>/opt/hostedtoolcache/Python/3.11.3/x64/lib/python3.11/site-packages/pyllamacpp/model.py</code> <pre><code>def generate(self,\n             prompt: str,\n             n_predict: Union[None, int] = None,\n             antiprompt: str = None,\n             infinite_generation: bool = False,\n             n_threads: int = 4,\n             repeat_last_n: int = 64,\n             top_k: int = 40,\n             top_p: float = 0.95,\n             temp: float = 0.8,\n             repeat_penalty: float = 1.10) -&gt; Generator:\n\"\"\"\n    Runs llama.cpp inference and yields new predicted tokens from the prompt provided as input\n\n    :param prompt: The prompt :)\n    :param n_predict: if n_predict is not None, the inference will stop if it reaches `n_predict` tokens, otherwise\n                      it will continue until `EOS`\n    :param antiprompt: aka the stop word, the generation will stop if this word is predicted,\n                       keep it None to handle it in your own way\n    :param infinite_generation: set it to `True` to make the generation go infinitely\n    :param n_threads: The number of CPU threads\n    :param repeat_last_n: last n tokens to penalize\n    :param top_k: top K sampling parameter\n    :param top_p: top P sampling parameter\n    :param temp: temperature\n    :param repeat_penalty: repeat penalty sampling parameter\n    :return: Tokens generator\n    \"\"\"\n    input_tokens = self._prompt_prefix_tokens + pp.llama_tokenize(self._ctx, prompt,\n                                                                  True) + self._prompt_suffix_tokens\n    if len(input_tokens) &gt; self._n_ctx - 4:\n        raise Exception('Prompt too long!')\n    predicted_tokens = []\n    predicted_token = 0\n\n    # add global context for the first time\n    if self._n_past == 0:\n        for tok in self._prompt_context_tokens:\n            predicted_tokens.append(tok)\n            self._last_n_tokens.pop(0)\n            self._last_n_tokens.append(tok)\n\n    # consume input tokens\n    for tok in input_tokens:\n        predicted_tokens.append(tok)\n        self._last_n_tokens.pop(0)\n        self._last_n_tokens.append(tok)\n\n    n_remain = 0\n    if antiprompt is not None:\n        sequence_queue = []\n        stop_word = antiprompt.strip()\n\n    while infinite_generation or predicted_token != pp.llama_token_eos():\n        if len(predicted_tokens) &gt; 0:\n            if (pp.llama_eval(self._ctx,\n                              predicted_tokens,\n                              len(predicted_tokens),\n                              self._n_past,\n                              n_threads)):\n                raise Exception(\"failed to eval the model!\")\n            self._n_past += len(predicted_tokens)\n            predicted_tokens.clear()\n\n        predicted_token = pp.llama_sample_top_p_top_k(self._ctx,\n                                                      self._last_n_tokens[self._n_ctx - repeat_last_n:],\n                                                      repeat_last_n,\n                                                      top_k,\n                                                      top_p,\n                                                      temp,\n                                                      repeat_penalty)\n\n        predicted_tokens.append(predicted_token)\n        # tokens come as raw undecoded bytes,\n        # and we decode them, replacing those that can't be decoded.\n        # i decoded here for fear of breaking the stopword logic, \n        token_str = pp.llama_token_to_str(self._ctx, predicted_token).decode('utf-8', \"replace\")\n        if antiprompt is not None:\n            if token_str == '\\n':\n                sequence_queue.append(token_str)\n                continue\n            if len(sequence_queue) != 0:\n                if stop_word.startswith(''.join(sequence_queue).strip()):\n                    sequence_queue.append(token_str)\n                    if ''.join(sequence_queue).strip() == stop_word:\n                        break\n                    else:\n                        continue\n                else:\n                    # consume sequence queue tokens\n                    while len(sequence_queue) != 0:\n                        yield sequence_queue.pop(0)\n                    sequence_queue = []\n        self._last_n_tokens.pop(0)\n        self._last_n_tokens.append(predicted_token)\n        yield token_str\n        if n_predict is not None:\n            if n_remain == n_predict:\n                break\n            else:\n                n_remain += 1\n</code></pre>"},{"location":"#pyllamacpp.model.Model.cpp_generate","title":"cpp_generate","text":"<pre><code>cpp_generate(\n    prompt,\n    n_predict=128,\n    new_text_callback=None,\n    n_threads=4,\n    repeat_last_n=64,\n    top_k=40,\n    top_p=0.95,\n    temp=0.8,\n    repeat_penalty=1.1,\n    n_batch=8,\n    n_keep=0,\n    interactive=False,\n    antiprompt=[],\n    ignore_eos=False,\n    instruct=False,\n    verbose_prompt=False,\n)\n</code></pre> <p>The generate function from <code>llama.cpp</code></p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>the prompt</p> required <code>n_predict</code> <code>int</code> <p>number of tokens to generate</p> <code>128</code> <code>new_text_callback</code> <code>Callable[[bytes], None]</code> <p>a callback function called when new text is generated, default <code>None</code></p> <code>None</code> <code>n_threads</code> <code>int</code> <p>The number of CPU threads</p> <code>4</code> <code>repeat_last_n</code> <code>int</code> <p>last n tokens to penalize</p> <code>64</code> <code>top_k</code> <code>int</code> <p>top K sampling parameter</p> <code>40</code> <code>top_p</code> <code>float</code> <p>top P sampling parameter</p> <code>0.95</code> <code>temp</code> <code>float</code> <p>temperature</p> <code>0.8</code> <code>repeat_penalty</code> <code>float</code> <p>repeat penalty sampling parameter</p> <code>1.1</code> <code>n_batch</code> <code>int</code> <p>GPT params n_batch</p> <code>8</code> <code>n_keep</code> <code>int</code> <p>GPT params n_keep</p> <code>0</code> <code>interactive</code> <code>bool</code> <p>interactive communication</p> <code>False</code> <code>antiprompt</code> <code>List</code> <p>list of anti prompts</p> <code>[]</code> <code>ignore_eos</code> <code>bool</code> <p>Ignore LLaMA EOS</p> <code>False</code> <code>instruct</code> <code>bool</code> <p>Activate instruct mode</p> <code>False</code> <code>verbose_prompt</code> <code>bool</code> <p>verbose prompt</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>the new generated text</p> Source code in <code>/opt/hostedtoolcache/Python/3.11.3/x64/lib/python3.11/site-packages/pyllamacpp/model.py</code> <pre><code>def cpp_generate(self, prompt: str,\n                 n_predict: int = 128,\n                 new_text_callback: Callable[[bytes], None] = None,\n                 n_threads: int = 4,\n                 repeat_last_n: int = 64,\n                 top_k: int = 40,\n                 top_p: float = 0.95,\n                 temp: float = 0.8,\n                 repeat_penalty: float = 1.10,\n                 n_batch: int = 8,\n                 n_keep: int = 0,\n                 interactive: bool = False,\n                 antiprompt: List = [],\n                 ignore_eos: bool = False,\n                 instruct: bool = False,\n                 verbose_prompt: bool = False,\n                 ) -&gt; str:\n\"\"\"\n    The generate function from `llama.cpp`\n\n    :param prompt: the prompt\n    :param n_predict: number of tokens to generate\n    :param new_text_callback: a callback function called when new text is generated, default `None`\n    :param n_threads: The number of CPU threads\n    :param repeat_last_n: last n tokens to penalize\n    :param top_k: top K sampling parameter\n    :param top_p: top P sampling parameter\n    :param temp: temperature\n    :param repeat_penalty: repeat penalty sampling parameter\n    :param n_batch: GPT params n_batch\n    :param n_keep: GPT params n_keep\n    :param interactive: interactive communication\n    :param antiprompt: list of anti prompts\n    :param ignore_eos: Ignore LLaMA EOS\n    :param instruct: Activate instruct mode\n    :param verbose_prompt: verbose prompt\n    :return: the new generated text\n    \"\"\"\n    self.gpt_params.prompt = prompt\n    self.gpt_params.n_predict = n_predict\n    # update other params if any\n    self.gpt_params.n_threads = n_threads\n    self.gpt_params.repeat_last_n = repeat_last_n\n    self.gpt_params.top_k = top_k\n    self.gpt_params.top_p = top_p\n    self.gpt_params.temp = temp\n    self.gpt_params.repeat_penalty = repeat_penalty\n    self.gpt_params.n_batch = n_batch\n    self.gpt_params.n_keep = n_keep\n    self.gpt_params.interactive = interactive\n    self.gpt_params.antiprompt = antiprompt\n    self.gpt_params.ignore_eos = ignore_eos\n    self.gpt_params.instruct = instruct\n    self.gpt_params.verbose_prompt = verbose_prompt\n\n    # assign new_text_callback\n    self.res = \"\"\n    Model._new_text_callback = new_text_callback\n\n    # run the prediction\n    pp.llama_generate(self._ctx, self.gpt_params, self._call_new_text_callback)\n    return self.res\n</code></pre>"},{"location":"#pyllamacpp.model.Model.get_params","title":"get_params  <code>staticmethod</code>","text":"<pre><code>get_params(params)\n</code></pre> <p>Returns a <code>dict</code> representation of the params</p> <p>Returns:</p> Type Description <code>dict</code> <p>params dict</p> Source code in <code>/opt/hostedtoolcache/Python/3.11.3/x64/lib/python3.11/site-packages/pyllamacpp/model.py</code> <pre><code>@staticmethod\ndef get_params(params) -&gt; dict:\n\"\"\"\n    Returns a `dict` representation of the params\n    :return: params dict\n    \"\"\"\n    res = {}\n    for param in dir(params):\n        if param.startswith('__'):\n            continue\n        res[param] = getattr(params, param)\n    return res\n</code></pre>"},{"location":"#pygptj.model","title":"pygptj.model","text":"<p>This module contains a simple Python API around gpt-j</p>"},{"location":"#pygptj.model.Model","title":"Model","text":"<pre><code>Model(\n    model_path,\n    prompt_context=\"\",\n    prompt_prefix=\"\",\n    prompt_suffix=\"\",\n    log_level=logging.ERROR,\n)\n</code></pre> <p>GPT-J model</p> <p>Example usage <pre><code>from pygptj.model import Model\n\nmodel = Model(ggml_model='path/to/ggml/model')\nfor token in model.generate(\"Tell me a joke ?\"):\n    print(token, end='', flush=True)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>The path to a gpt-j <code>ggml</code> model</p> required <code>prompt_context</code> <code>str</code> <p>the global context of the interaction</p> <code>''</code> <code>prompt_prefix</code> <code>str</code> <p>the prompt prefix</p> <code>''</code> <code>prompt_suffix</code> <code>str</code> <p>the prompt suffix</p> <code>''</code> <code>log_level</code> <code>int</code> <p>logging level</p> <code>logging.ERROR</code> Source code in <code>/opt/hostedtoolcache/Python/3.11.3/x64/lib/python3.11/site-packages/pygptj/model.py</code> <pre><code>def __init__(self,\n             model_path: str,\n             prompt_context: str = '',\n             prompt_prefix: str = '',\n             prompt_suffix: str = '',\n             log_level: int = logging.ERROR):\n\"\"\"\n    :param model_path: The path to a gpt-j `ggml` model\n    :param prompt_context: the global context of the interaction\n    :param prompt_prefix: the prompt prefix\n    :param prompt_suffix: the prompt suffix\n    :param log_level: logging level\n    \"\"\"\n    # set logging level\n    set_log_level(log_level)\n    self._ctx = None\n\n    if not Path(model_path).is_file():\n        raise Exception(f\"File {model_path} not found!\")\n\n    self.model_path = model_path\n\n    self._model = pp.gptj_model()\n    self._vocab = pp.gpt_vocab()\n\n    # load model\n    self._load_model()\n\n    # gpt params\n    self.gpt_params = pp.gptj_gpt_params()\n    self.hparams = pp.gptj_hparams()\n\n    self.res = \"\"\n\n    self.logits = []\n\n    self._n_past = 0\n    self.prompt_cntext = prompt_context\n    self.prompt_prefix = prompt_prefix\n    self.prompt_suffix = prompt_suffix\n\n    self._prompt_context_tokens = []\n    self._prompt_prefix_tokens = []\n    self._prompt_suffix_tokens = []\n\n    self.reset()\n</code></pre>"},{"location":"#pygptj.model.Model.generate","title":"generate","text":"<pre><code>generate(\n    prompt,\n    n_predict=None,\n    antiprompt=None,\n    seed=None,\n    n_threads=4,\n    top_k=40,\n    top_p=0.9,\n    temp=0.9,\n)\n</code></pre> <p>Runs GPT-J inference and yields new predicted tokens</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The prompt :)</p> required <code>n_predict</code> <code>Union[None, int]</code> <p>if n_predict is not None, the inference will stop if it reaches <code>n_predict</code> tokens, otherwise it will continue until <code>end of text</code> token</p> <code>None</code> <code>antiprompt</code> <code>str</code> <p>aka the stop word, the generation will stop if this word is predicted, keep it None to handle it in your own way</p> <code>None</code> <code>seed</code> <code>int</code> <p>random seed</p> <code>None</code> <code>n_threads</code> <code>int</code> <p>The number of CPU threads</p> <code>4</code> <code>top_k</code> <code>int</code> <p>top K sampling parameter</p> <code>40</code> <code>top_p</code> <code>float</code> <p>top P sampling parameter</p> <code>0.9</code> <code>temp</code> <code>float</code> <p>temperature</p> <code>0.9</code> <p>Returns:</p> Type Description <code>Generator</code> <p>Tokens generator</p> Source code in <code>/opt/hostedtoolcache/Python/3.11.3/x64/lib/python3.11/site-packages/pygptj/model.py</code> <pre><code>def generate(self,\n             prompt: str,\n             n_predict: Union[None, int] = None,\n             antiprompt: str = None,\n             seed: int = None,\n             n_threads: int = 4,\n             top_k: int = 40,\n             top_p: float = 0.9,\n             temp: float = 0.9,\n             ) -&gt; Generator:\n\"\"\"\n     Runs GPT-J inference and yields new predicted tokens\n\n    :param prompt: The prompt :)\n    :param n_predict: if n_predict is not None, the inference will stop if it reaches `n_predict` tokens, otherwise\n                      it will continue until `end of text` token\n    :param antiprompt: aka the stop word, the generation will stop if this word is predicted,\n                       keep it None to handle it in your own way\n    :param seed: random seed\n    :param n_threads: The number of CPU threads\n    :param top_k: top K sampling parameter\n    :param top_p: top P sampling parameter\n    :param temp: temperature\n\n    :return: Tokens generator\n    \"\"\"\n    if seed is None or seed &lt; 0:\n        seed = int(time.time())\n\n    logging.info(f'seed = {seed}')\n\n    if self._n_past == 0 or antiprompt is None:\n        # add the prefix to the context\n        embd_inp = self._prompt_prefix_tokens + pp.gpt_tokenize(self._vocab, prompt) + self._prompt_suffix_tokens\n    else:\n        # do not add the prefix again as it is already in the previous generated context\n        embd_inp = pp.gpt_tokenize(self._vocab, prompt) + self._prompt_suffix_tokens\n\n    if n_predict is not None:\n        n_predict = min(n_predict, self.hparams.n_ctx - len(embd_inp))\n    logging.info(f'Number of tokens in prompt = {len(embd_inp)}')\n\n    embd = []\n    # add global context for the first time\n    if self._n_past == 0:\n        for tok in self._prompt_context_tokens:\n            embd.append(tok)\n\n    # consume input tokens\n    for tok in embd_inp:\n        embd.append(tok)\n\n    # determine the required inference memory per token:\n    mem_per_token = 0\n    logits, mem_per_token = pp.gptj_eval(self._model, n_threads, 0, [0, 1, 2, 3], mem_per_token)\n\n    i = len(embd) - 1\n    id = 0\n    if antiprompt is not None:\n        sequence_queue = []\n        stop_word = antiprompt.strip()\n\n    while id != 50256:  # end of text token\n        if n_predict is not None:  # break the generation if n_predict\n            if i &gt;= (len(embd_inp) + n_predict):\n                break\n        i += 1\n        # predict\n        if len(embd) &gt; 0:\n            try:\n                logits, mem_per_token = pp.gptj_eval(self._model, n_threads, self._n_past, embd, mem_per_token)\n                self.logits.append(logits)\n            except Exception as e:\n                print(f\"Failed to predict\\n {e}\")\n                return\n\n        self._n_past += len(embd)\n        embd.clear()\n\n        if i &gt;= len(embd_inp):\n            # sample next token\n            n_vocab = self.hparams.n_vocab\n            t_start_sample_us = int(round(time.time() * 1000000))\n            id = pp.gpt_sample_top_k_top_p(self._vocab, logits[-n_vocab:], top_k, top_p, temp, seed)\n            if id == 50256:  # end of text token\n                break\n            # add the token to the context\n            embd.append(id)\n            token = self._vocab.id_to_token[id]\n            # antiprompt\n            if antiprompt is not None:\n                if token == '\\n':\n                    sequence_queue.append(token)\n                    continue\n                if len(sequence_queue) != 0:\n                    if stop_word.startswith(''.join(sequence_queue).strip()):\n                        sequence_queue.append(token)\n                        if ''.join(sequence_queue).strip() == stop_word:\n                            break\n                        else:\n                            continue\n                    else:\n                        # consume sequence queue tokens\n                        while len(sequence_queue) != 0:\n                            yield sequence_queue.pop(0)\n                        sequence_queue = []\n\n            yield token\n</code></pre>"},{"location":"#pygptj.model.Model.cpp_generate","title":"cpp_generate","text":"<pre><code>cpp_generate(\n    prompt,\n    new_text_callback=None,\n    logits_callback=None,\n    n_predict=128,\n    seed=-1,\n    n_threads=4,\n    top_k=40,\n    top_p=0.9,\n    temp=0.9,\n    n_batch=8,\n)\n</code></pre> <p>Runs the inference to cpp generate function</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>the prompt</p> required <code>new_text_callback</code> <code>Callable[[str], None]</code> <p>a callback function called when new text is generated, default <code>None</code></p> <code>None</code> <code>logits_callback</code> <code>Callable[[np.ndarray], None]</code> <p>a callback function to access the logits on every inference</p> <code>None</code> <code>n_predict</code> <code>int</code> <p>number of tokens to generate</p> <code>128</code> <code>seed</code> <code>int</code> <p>The random seed</p> <code>-1</code> <code>n_threads</code> <code>int</code> <p>Number of threads</p> <code>4</code> <code>top_k</code> <code>int</code> <p>top_k sampling parameter</p> <code>40</code> <code>top_p</code> <code>float</code> <p>top_p sampling parameter</p> <code>0.9</code> <code>temp</code> <code>float</code> <p>temperature sampling parameter</p> <code>0.9</code> <code>n_batch</code> <code>int</code> <p>batch size for prompt processing</p> <code>8</code> <p>Returns:</p> Type Description <code>str</code> <p>the new generated text</p> Source code in <code>/opt/hostedtoolcache/Python/3.11.3/x64/lib/python3.11/site-packages/pygptj/model.py</code> <pre><code>def cpp_generate(self,\n                 prompt: str,\n                 new_text_callback: Callable[[str], None] = None,\n                 logits_callback: Callable[[np.ndarray], None] = None,\n                 n_predict: int = 128,\n                 seed: int = -1,\n                 n_threads: int = 4,\n                 top_k: int = 40,\n                 top_p: float = 0.9,\n                 temp: float = 0.9,\n                 n_batch: int = 8,\n                 ) -&gt; str:\n\"\"\"\n    Runs the inference to cpp generate function\n\n    :param prompt: the prompt\n    :param new_text_callback: a callback function called when new text is generated, default `None`\n    :param logits_callback: a callback function to access the logits on every inference\n    :param n_predict: number of tokens to generate\n    :param seed: The random seed\n    :param n_threads: Number of threads\n    :param top_k: top_k sampling parameter\n    :param top_p: top_p sampling parameter\n    :param temp: temperature sampling parameter\n    :param n_batch: batch size for prompt processing\n\n    :return: the new generated text\n    \"\"\"\n    self.gpt_params.prompt = prompt\n    self.gpt_params.n_predict = n_predict\n    self.gpt_params.seed = seed\n    self.gpt_params.n_threads = n_threads\n    self.gpt_params.top_k = top_k\n    self.gpt_params.top_p = top_p\n    self.gpt_params.temp = temp\n    self.gpt_params.n_batch = n_batch\n\n    # assign new_text_callback\n    self.res = \"\"\n    Model._new_text_callback = new_text_callback\n\n    # assign _logits_callback used for saving logits, token by token\n    Model._logits_callback = logits_callback\n\n    # run the prediction\n    pp.gptj_generate(self.gpt_params, self._model, self._vocab, self._call_new_text_callback,\n                     self._call_logits_callback)\n    return self.res\n</code></pre>"},{"location":"#pygptj.model.Model.braindump","title":"braindump","text":"<pre><code>braindump(path)\n</code></pre> <p>Dumps the logits to .npy</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Output path</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>/opt/hostedtoolcache/Python/3.11.3/x64/lib/python3.11/site-packages/pygptj/model.py</code> <pre><code>def braindump(self, path: str) -&gt; None:\n\"\"\"\n    Dumps the logits to .npy\n    :param path: Output path\n    :return: None\n    \"\"\"\n    np.save(path, np.asarray(self.logits))\n</code></pre>"},{"location":"#pygptj.model.Model.reset","title":"reset","text":"<pre><code>reset()\n</code></pre> <p>Resets the context</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>/opt/hostedtoolcache/Python/3.11.3/x64/lib/python3.11/site-packages/pygptj/model.py</code> <pre><code>def reset(self) -&gt; None:\n\"\"\"\n    Resets the context\n    :return: None\n    \"\"\"\n    self._n_past = 0\n    self._prompt_context_tokens = pp.gpt_tokenize(self._vocab, self.prompt_cntext)\n    self._prompt_prefix_tokens = pp.gpt_tokenize(self._vocab, self.prompt_prefix)\n    self._prompt_suffix_tokens = pp.gpt_tokenize(self._vocab, self.prompt_suffix)\n</code></pre>"},{"location":"#pygptj.model.Model.get_params","title":"get_params  <code>staticmethod</code>","text":"<pre><code>get_params(params)\n</code></pre> <p>Returns a <code>dict</code> representation of the params</p> <p>Returns:</p> Type Description <code>dict</code> <p>params dict</p> Source code in <code>/opt/hostedtoolcache/Python/3.11.3/x64/lib/python3.11/site-packages/pygptj/model.py</code> <pre><code>@staticmethod\ndef get_params(params) -&gt; dict:\n\"\"\"\n    Returns a `dict` representation of the params\n    :return: params dict\n    \"\"\"\n    res = {}\n    for param in dir(params):\n        if param.startswith('__'):\n            continue\n        res[param] = getattr(params, param)\n    return res\n</code></pre>"}]}